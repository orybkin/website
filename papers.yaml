papers:
  - name: LEXA
    title:
      link: https://arxiv.org/pdf/2110.09514.pdf
      name: Discovering and Achieving Goals via World Models
    authors:
      - name: Russell Mendonca
        href: https://russellmendonca.github.io/
        star: true
      - me: true
        star: true
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Danijar Hafner
        href: https://danijar.com/
      - name: Deepak Pathak
        href: https://www.cs.cmu.edu/~dpathak/
    conference:
      name: Neural Information Processing Systems (NeurIPS)
      year: 2021 </br> <i>ICML Workshop on Unsupervised RL</i>, 2021 (<oral>oral presentation</oral>, 6% acceptance rate) </br> <i>ICML Workshop on Self-Supervised Learning</i>, 2021 (<oral>oral presentation</oral>)
      # URL is 3 out of 47 submissions, 6.3%
      # The other workshop has 13% acceptance rate or lower: 7 out of 52 accepted posters
    links:
      - name: project page & videos
        href: https://orybkin.github.io/lexa/
      - name: arXiv
        href: https://arxiv.org/abs/2110.09514
      - name: poster
        href: images/lexa_poster.pdf
      - name: video (2 min)
        href: https://www.youtube.com/watch?v=LnZj2lZYD3k
      - name: talk (13 min)
        href: https://www.youtube.com/watch?v=WWHlQbigQp4
      - name: code
        href: https://github.com/orybkin/lexa
    description: Latent Explorer Achiever (LEXA) discovers visual goals and learns to reach them in imagination, without any supervision. With forward-looking exploration, LEXA solves hard tasks previously only solved with demonstrations or rewards.
    highlighted: true
    nogif: false

  - name: roboaware
    title:
      link: https://arxiv.org/pdf/2107.09047.pdf
      name: "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness"
    authors:
      - name: Edward S. Hu
        href: https://edwardshu.com/
      - name: Kun Huang
        href: https://voyager1998.github.io/
      - me: true
      - name: Dinesh Jayaraman
        href: https://www.seas.upenn.edu/~dineshj/
    conference:
      name: Preprint
      year: 2021
    links:
      - name: project page & videos
        href: https://hueds.github.io/rac/
      - name: arXiv
        href: https://arxiv.org/abs/2107.09047
      - name: talk (5 min)
        href: https://www.youtube.com/watch?v=Q_9u6B54iAc
    description: Through self-awareness, our robots learn separate robot and world dynamics, enabling better cost functions for visual MBRL and zero-shot transfer from a single robot to multiple other robots.

  - name: latco
    title:
      link: https://arxiv.org/pdf/2106.13229.pdf
      name: Model-Based Reinforcement Learning via Latent-Space Collocation
    authors:
      - me: true
        star: true
      - name: Chuning Zhu
        href: https://www.seas.upenn.edu/~zchuning/
        star: true
      - name: Anusha Nagabandi
        href: https://www.linkedin.com/in/anusha-nagabandi-a4923bba
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Igor Mordatch
        href: https://twitter.com/imordatch
      - name: Sergey Levine
        href: https://people.eecs.berkeley.edu/~svlevine
    conference:
      name: International Conference on Machine Learning (ICML)
      year: 2021
    links:
      - name: project page & videos
        href: https://orybkin.github.io/latco/
      - name: arXiv
        href: https://arxiv.org/abs/2106.13229
      - name: poster
        href: images/latco_poster.pdf
      - name: talk (5 min)
        href: https://www.youtube.com/watch?v=skc0e4KYNcw
      - name: code
        href: https://github.com/zchuning/latco
    description: Our planner, LatCo, solves multi-stage long-horizon tasks where prior work fails. By optimizing a sequence of future latent states instead of optimizing actions directly, it quickly discovers the high-reward region to create effective visual plans.
    highlighted: true

  - name: sigma
    title:
      link: https://arxiv.org/pdf/2006.13202.pdf
      name: Simple and Effective VAE Training with Calibrated Decoders
    authors:
      - me: true
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Sergey Levine
        href: https://people.eecs.berkeley.edu/~svlevine
    conference:
      name: International Conference on Machine Learning (ICML)
      year: 2021
    links:
      - name: project page & videos
        href: https://orybkin.github.io/sigma-vae/
      - name: arXiv
        href: https://arxiv.org/abs/2006.13202
      - name: poster
        href: images/sigma_poster.pdf
      - name: talk (5 min)
        href: https://www.youtube.com/watch?v=6UMzH68RwGM
      - name: code
        href: https://github.com/orybkin/sigma-vae
    description: Commonly used uncalibrated decoders adversely affect VAE training. However, learning appropriate calibrated decoders yields better samples, is simple to implement, and does not require the common heuristic weight on the KL divergence.
    highlighted: true

  - name: RLV
    title:
      link: https://arxiv.org/pdf/2011.06507.pdf
      name: "Reinforcement Learning with Videos: Combining Offline Observations with Interaction"
    authors:
      - name: Karl Schmeckpeper
        hre: https://sites.google.com/view/karlschmeckpeper
      - me: true
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Sergey Levine
        href: https://people.eecs.berkeley.edu/~svlevine
      - name: Chelsea Finn
        href: https://ai.stanford.edu/~cbfinn/
    conference:
      name: Conference on Robot Learning (CoRL)
      year: 2020 (<oral>oral presentation</oral>, 4% acceptance rate)
    links:
      - name: project page & videos
        href: https://sites.google.com/view/rl-with-videos
      - name: arXiv
        href: https://arxiv.org/abs/2011.06507
      - name: talk (5 min)
        href: https://www.youtube.com/watch?v=aIWr4fhzPFA
      - name: code
        href: https://github.com/kschmeckpeper/rl_with_videos
    description: We use offline observations of humans jointly with online robot interaction data in a joint reinforcement learning algortihm. The resulting approach is able to learn from real-world human videos to solve challenging robotic tasks.

  - name: GCP
    title:
      link: https://arxiv.org/pdf/2006.13205.pdf
      name: Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors
    authors:
      - name: Karl Pertsch
        href: https://kpertsch.github.io/
        star: true
      - me: true
        star: true
      - name: Frederik Ebert
        href: https://febert.github.io
      - name: Chelsea Finn
        href: https://ai.stanford.edu/~cbfinn/
      - name: Dinesh Jayaraman
        href: https://www.seas.upenn.edu/~dineshj
      - name: Sergey Levine
        href: https://people.eecs.berkeley.edu/~svlevine
    conference:
      name: Neural Information Processing Systems (NeurIPS)
      year: 2020
    links:
      - name: project page & videos
        href: https://orybkin.github.io/video-gcp/
      - name: arXiv
        href: https://arxiv.org/abs/2006.13205
      - name: video (1 min)
        href: https://www.youtube.com/watch?v=axXx-x86IeY&feature=emb_logo
      - name: talk (5 min)
        href: https://youtu.be/w32twGTWvDU
      - name: code
        href: https://github.com/orybkin/video-gcp
    description: Using hierarchical goal-conditioned predictive models, we scale to very long horizon prediction of more than 500 frames. The model is also useful for hierarchical visual planning and long-horizon control.
    highlighted: true

  - name: SSA
    title:
      link: https://arxiv.org/pdf/1912.12773.pdf
      name: Learning Predictive Models From Observation and Interaction
    authors:
      - name: Karl Schmeckpeper
        hre: https://sites.google.com/view/karlschmeckpeper
      - name: Annie Xie
        href: https://anxie.github.io
      - me: true
      - name: Stephen Tian
        href: https://s-tian.github.io
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Sergey Levine
        href: https://people.eecs.berkeley.edu/~svlevine
      - name: Chelsea Finn
        href: https://ai.stanford.edu/~cbfinn/
    conference:
      name: European Conference on Computer Vision (ECCV)
      year: 2020
    links:
      - name: project page & videos
        href: https://sites.google.com/view/lpmfoai
      - name: arXiv
        href: https://arxiv.org/abs/1912.12773
      - name: video (1 minute)
        href: https://www.youtube.com/watch?v=jWbwh4uZFgU
      - name: talk (5 min)
        href: https://www.youtube.com/watch?v=V_yLQtnS5YE
      - name: workshop version
        href: https://drive.google.com/file/d/1gE8hXP6NEFpoUtpAPH60NkjrzEjCc-Sj/view
    description:  We learn action representations that generalize between robot data and passive observations of other agents (e.g. humans). This enables the use of additional diverse sources of data to train models for visual robotic control.
    highlighted: true

  - name: P2E
    title:
      link: https://arxiv.org/pdf/2005.05960.pdf
      name: Planning to Explore via Self-Supervised World Models
    authors:
      - name: Ramanan Sekar
        href: https://ramanans1.github.io/
        star: true
      - me: true
        star: true
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Pieter Abbeel
        href: https://people.eecs.berkeley.edu/~pabbeel/
      - name: Danijar Hafner
        href: https://danijar.com/
      - name: Deepak Pathak
        href: https://www.cs.cmu.edu/~dpathak/
    conference:
      name: International Conference on Machine Learning (ICML)
      year: 2020
    links:
      - name: project page & videos
        href: https://ramanans1.github.io/plan2explore/
      - name: arXiv
        href: https://arxiv.org/abs/2005.05960
      - name: video (2 min)
        href: https://www.youtube.com/watch?v=GftqnPWsCWw&feature=emb_logo
      - name: talk (10 min)
        href: https://www.youtube.com/watch?v=gan79mAVfq8
      - name: VentureBeat
        href: https://venturebeat.com/2020/05/13/plan2explore-adapts-to-exploration-tasks-without-fine-tuning/
      - name: blog
        href: https://blog.ml.cmu.edu/2020/10/06/plan2explore/
      - name: code
        href: https://github.com/ramanans1/plan2explore
    description: By planning to explore, as opposed to exploring retrospectively, we significantly improve unsupervised agents. Our agent is able to adapt in a zero/few-shot setup from images, achieving comparable performance to the many-shot oracle.
    highlighted: true

  - name: KeyIn
    title:
      link: https://arxiv.org/pdf/1904.05869.pdf
      name: "Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning"
    authors:
      - name: Karl Pertsch
        href: https://kpertsch.github.io/
        star: true
      - me: true
        star: true
      - name: Jingyun Yang
        href: https://www.linkedin.com/in/yjy0625
      - name: Shenghao Zhou
        href: https://github.com/AZdet
      - name: Kosta Derpanis
        href: http://www.scs.ryerson.ca/kosta/
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Joseph Lim
        href: http://www-bcf.usc.edu/~limjj/
      - name: Andrew Jaegle
        href: http://www.drewjaegle.com/
    conference:
      name: Conference on Learning for Dynamics and Control (L4DC)
      year: 2020
    links:
      - name: project page & videos
        href: https://sites.google.com/view/keyin
      - name: arXiv
        href: https://arxiv.org/abs/1904.05869
      - name: poster
        href: images/KeyIn_poster.pdf
      - name: slides
        href: https://docs.google.com/presentation/d/154jEGsWOStl46DvZvam3HVTGyi8KslLy8lP6ClkiEE8/edit?usp=sharing
      - name: talk (5 min)
        href: https://www.youtube.com/watch?v=e2hVV5FDKf8&feature=emb_logo
    description: We discover keyframes in videos by learning to select frames that enable prediction of the entire sequence. By using the keyframe structure of the data for prediction, our method is further able to perform planning for longer horizons.

  - name: CLASP
    title:
      link: https://openreview.net/pdf?id=SylPMnR9Ym
      name: Learning what you can do before doing anything
    authors:
      - me: true
        star: true
      - name: Karl Pertsch
        href: https://kpertsch.github.io/
        star: true
      - name: Kosta Derpanis
        href: http://www.scs.ryerson.ca/kosta/
      - name: Kostas Daniilidis
        href: http://www.cis.upenn.edu/~kostas
      - name: Andrew Jaegle
        href: http://www.drewjaegle.com/
    conference:
      name: International Conference on Learning Representations (ICLR)
      year: 2019
    links:
      - name: project page & videos
        href: https://daniilidis-group.github.io/learned_action_spaces/
      - name: paper
        href: https://openreview.net/pdf?id=SylPMnR9Ym
      - name: poster
        href: images/CLASP_poster.pdf
      - name: slides
        href: https://docs.google.com/presentation/d/1sGfvcLdEl_BAlPqtoTPchxKNnBDiiCWuzRSz4lWH07E/edit?usp=sharing
    description: We learn to discover an agent's action space along with a dynamics model from pure video data. The model can be used for model predictive control, requiring orders of magnitude fewer action-annotated videos than other methods.
    nogif: true

  - name: pixel_metrics
    title:
      link: https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d
      name: The reasonable ineffectiveness of pixel metrics for future prediction
    authors:
      - me: true
    conference:
      name: Blog post
      year: 2018
    links:
    description: MSE loss and its variants are commonly used for training and evaluation of future prediction. But is this the right thing to do?
    highlighted: false
